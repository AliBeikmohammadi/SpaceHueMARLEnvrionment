import pygame
import pymunk
import random
from pettingzoo.utils import wrappers
from pettingzoo import AECEnv
from gymnasium import spaces
import numpy as np

#Imports like WW. Implements agent_selector, but has parallel wrappers
from gymnasium.utils import EzPickle
from pettingzoo.utils import agent_selector, wrappers
from pettingzoo.utils.conversions import parallel_wrapper_fn

#Some more imports ??
from gymnasium.utils import seeding
from scipy.spatial import distance as ssd


#Section from WW for wrappers
# def env(**kwargs):
#     env = PursuersEnv(**kwargs)
#     env = wrappers.ClipOutOfBoundsWrapper(env)
#     env = wrappers.OrderEnforcingWrapper(env)
#     return env

# parallel_env = parallel_wrapper_fn(env)



class MovingObject:
    def __init__(self, x, y, pixel_scale=750, radius=0.015):
        self.pixel_scale = 30 * 25
        self.body = pymunk.Body()
        self.body.position = x, y

        self.shape = pymunk.Circle(self.body, pixel_scale * radius)
        self.shape.elasticity = 1
        self.shape.density = 1
        self.shape.custom_value = 1

        self.shape.reset_position = self.reset_position
        self.shape.reset_velocity = self.reset_velocity

        self.radius = radius * pixel_scale

    def add(self, space):
        space.add(self.body, self.shape)

    def draw(self, display, convert_coordinates):
        pygame.draw.circle(
            display, self.color, convert_coordinates(self.body.position), self.radius
        )

    def reset_position(self, x, y):
        self.body.position = x, y

    def reset_velocity(self, vx, vy):
        self.body.velocity = vx, vy


class Pursuers(MovingObject):
    def __init__(self, x, y, max_accel, pursuer_speed, radius=0.015, sensor_length=None, sensor_count=30):
        super().__init__(x, y, radius=radius)

        self.color = (101, 104, 249)
        self.max_accel = max_accel
        self.max_speed = pursuer_speed
        self.body.velocity = 0.0, 0.0

        # If no sensor_length is specified, make it 5 times the radius
        if sensor_length is None:
            sensor_length = 5 * radius

        # Add a star pattern sensor
        self.sensor_count = sensor_count
        self.sensor_data = [False] * sensor_count
        self.sensors = []
        self.sensor_length = sensor_length * self.pixel_scale  # scale the sensor_length
        for i in range(sensor_count):
            angle = i * 2 * np.pi / sensor_count
            end_x = self.sensor_length * np.cos(angle)
            end_y = self.sensor_length * np.sin(angle)
            sensor = pymunk.Segment(self.body, (0, 0), (end_x, end_y), 1)
            sensor.sensor = True
            sensor.collision_type = 2  # different from the body's collision type
            self.sensors.append(sensor)
        
    def take_action(self, action):
        # Scale velocity by action
        self.body.velocity = (self.max_speed * action[0], self.max_speed * action[1])

    def get_observation(self):
        # Return position, velocity, and sensor data
        return np.concatenate((self.position, self.velocity, self.sensor_data))


    def add(self, space):
        super().add(space)
        for sensor in self.sensors:
            space.add(sensor)  # add sensor to the space

    def draw(self, display, convert_coordinates):
        super().draw(display, convert_coordinates)
        # Draw the sensor lines
        for sensor in self.sensors:
            direction = sensor.b - sensor.a
            end_point = self.body.position + direction
            pygame.draw.line(display, self.color, convert_coordinates(self.body.position), convert_coordinates(end_point), 1)

    @property
    def observation_space(self):
        # Include sensor data in observation space
        return spaces.Box(
            low=np.zeros(2 + self.sensor_count),  # position, velocity, and sensor data
            high=np.ones(2 + self.sensor_count),
            dtype=np.float32,
        )

    @property
    def action_space(self):
        return spaces.Box(
            low=np.float32(-self.max_accel),
            high=np.float32(self.max_accel),
            shape=(2,),
            dtype=np.float32,
        )

    @property
    def position(self):
        assert self.body.position is not None
        return np.array([self.body.position[0], self.body.position[1]])

    @property
    def velocity(self):
        assert self.body.velocity is not None
        return np.array([self.body.velocity[0], self.body.velocity[1]])
    
class RewardTracker:
    def __init__(self):
        self.scores = {}

    def coin_collision(self, arbiter, space, data):
        self.scores[arbiter.shapes[0].body] = 10  # Increase the score when a collision happens
        space.remove(arbiter.shapes[1])  # remove the coin from the space
        return False  # prevent the collision from happening

    def get_score(self, body):
        return self.scores.get(body, 0)


class Coin(MovingObject):
    def __init__(self, x, y, radius=0.015):
        super().__init__(x, y, radius=radius)
        self.color = (255, 255, 0)  # Yellow color

    def draw(self, display, convert_coordinates):
        super().draw(display, convert_coordinates)

    def reset_velocity(self, vx, vy):
        # Overriding this method to ensure the coin cannot move
        pass
    
class PursuersEnv(AECEnv):
    metadata = {'render.modes': ['human'], "name": "pursuers-v0"}

    def __init__(self, num_pursuers, num_coins=3):
        self.num_pursuers = num_pursuers
        self.agents = [f'pursuer_{i}' for i in range(num_pursuers)]
        self.possible_agents = self.agents.copy()
        self.agent_name_to_agent = {}  # initialize dictionary to store agent objects

        # Define action and observation spaces
        # These are placeholders and should be replaced with your actual spaces
        self.action_spaces = {agent: spaces.Box(low=-1, high=1, shape=(2,)) for agent in self.agents}
        self.observation_spaces = {agent: spaces.Box(low=0, high=1, shape=(10,)) for agent in self.agents}
        
        self.rewards = {agent: 0 for agent in self.agents}
        self.dones = {agent: False for agent in self.agents}
        self.infos = {agent: {} for agent in self.agents}

        # Initialize Pygame and Pymunk
        pygame.init()
        self.space = pymunk.Space()
        self.screen_width, self.screen_height = 600, 400
        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))

        # Initialize Pursuers
        self.pursuers = []
        for i in range(num_pursuers):
            x = random.randint(50, self.screen_width - 50)
            y = random.randint(50, self.screen_height - 50)
            pursuer = Pursuers(x, y, max_accel=0.01, pursuer_speed=1)
            pursuer.add(self.space)
            self.pursuers.append(pursuer)
            self.agent_name_to_agent[f'pursuer_{i}'] = pursuer

        # Initialize RewardTracker
        self.reward_tracker = RewardTracker()
        handler = self.space.add_collision_handler(1, 1)  # assuming all pursuers have the same collision type
        handler.begin = self.reward_tracker.coin_collision

        self._cumulative_rewards = {agent: 0 for agent in self.agents}

        # Initialize Coins
        self.coins = []
        for _ in range(num_coins):
            x = random.randint(50, self.screen_width - 50)
            y = random.randint(50, self.screen_height - 50)
            coin = Coin(x, y)
            coin.add(self.space)
            self.coins.append(coin)


        self._agent_selector = agent_selector(self.agents)
        # create the agent selector
        for agent in self.agents:
            action = self.action_spaces[agent].sample()
            obs, reward, done, info = self.step({agent: action})
            if done[agent]:
                running = False

    
    def _clear_dead(self):
        for agent in self.agents:
            if self.dones[agent]:  # If an agent's done condition is True
                self.agents.remove(agent)  # remove it from the list of active agents

    def observe(self, agent):
        # Return the observation of a specific agent
        return self.agent_name_to_agent[agent].get_observation()
    
    def get_observation(self, agent):
        # Return the observation for a particular agent
        return self.agent_name_to_agent[agent].get_observation()

    def step(self, actions):
        # Update the environment state based on each agent's action
        for agent, action in actions.items():
            self.agent_name_to_agent[agent].take_action(action)

        # Update physics
        dt = 1/60.0  # 60 FPS
        self.space.step(dt)  # Move 1 step 

        # Compute reward, done, and info for this agent
        for agent in self.agents:
            self.rewards[agent], self.dones[agent], self.infos[agent] = self.compute_reward_done_info(agent)

        # Update velocities based on actions
        for agent, action in actions.items():
            self.agent_name_to_agent[agent].reset_velocity(action[0], action[1])

        # Select the next agent
        current_agent = self._agent_selector.next()

        self._clear_dead()

        return {current_agent: self.get_observation(current_agent)}, self.rewards, self.dones, self.infos
    

    def render(self, mode='human'):
        # Clear screen
        self.screen.fill((255, 255, 255))

        # Draw Pursuers
        for pursuer in self.pursuers:
            pursuer.draw(self.screen, self.convert_coordinates)

        # Draw Coins
        for coin in self.coins:
            coin.draw(self.screen, self.convert_coordinates)

        # Flip display
        pygame.display.flip()


    def convert_coordinates(self, position):
        return int(position[0]), self.screen_height - int(position[1])

    def close(self):
        pygame.quit()

    def reset(self):
        # Reset the environment to its initial state
        self.agents = self.possible_agents[:]
        self.rewards = {agent: 0 for agent in self.agents}
        self.dones = {agent: False for agent in self.agents}
        self.infos = {agent: {} for agent in self.agents}

        # Reset the position and velocity of each pursuer
        for pursuer in self.pursuers:
            x = random.randint(50, self.screen_width - 50)
            y = random.randint(50, self.screen_height - 50)
            pursuer.reset_position(x, y)
            pursuer.reset_velocity(0.0, 0.0)

        # Reset the position of each coin
        for coin in self.coins:
            x = random.randint(50, self.screen_width - 50)
            y = random.randint(50, self.screen_height - 50)
            coin.reset_position(x, y)

    def last(self):
        # Returns the last agent that took an action
        return self._agent_selector.previous()
    

    def _accumulate_rewards(self):
        # Accumulate rewards for each agent
        for agent in self.agents:
            self._cumulative_rewards[agent] += self.rewards[agent]

    def compute_reward_done_info(self, agent):
        # Compute the reward, done, and info for an agent after it takes an action
        reward = self.reward_tracker.get_score(self.agent_name_to_agent[agent].body)  # get current score as reward
        reward = reward if reward is not None else 0  # handle case where agent is not in reward tracker
        done = self._is_done(agent)  # compute done
        info = {}  # placeholder - you may want to include additional info
        return reward, done, info
    
    def _is_done(self, agent):
        # Define the condition under which an agent's episode is done
        # Here, we just return False, but you should define your own condition
        return False
    
    @property
    def observation_space(self):
        return spaces.Box(
            low=np.zeros(2 + self.sensor_count),  # position, velocity, and sensor data
            high=np.ones(2 + self.sensor_count),
            dtype=np.float32,
        )

    @property
    def action_space(self):
        return spaces.Box(
            low=np.float32(-self.max_accel),
            high=np.float32(self.max_accel),
            shape=(2,),
            dtype=np.float32,
        )
    
    @property
    def agent_selection(self):
        return self._agent_selector.current
    

    
# create the environment
environment = PursuersEnv(num_pursuers=2, num_coins=3)

running = True
while running:
    # Check for QUIT event
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False

    for agent in environment.agents:
        action = environment.action_spaces[agent].sample()
        obs, reward, done, info = environment.step({agent: action})
        if done[agent]:
            running = False
    environment.render()

    pygame.time.delay(100)  # delay for 100 ms
environment.close()
